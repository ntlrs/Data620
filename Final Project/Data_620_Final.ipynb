{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Yelp Inc. (NYSE: YELP) connects people with great local businesses. Yelp was founded in San Francisco in July 2004. Since then, Yelp communities have taken root in major metros across 32 countries. By the end of Q4 2018, Yelpers had written approximately 177 million rich, local reviews, making Yelp the leading local guide for real word-of-mouth on everything from boutiques and mechanics to restaurants and dentists. Approximately 33 million unique devices* accessed Yelp via the Yelp app, approximately 69 million unique visitors visited Yelp via mobile web** and approximately 62 million unique visitors visited Yelp via desktop*** on a monthly average basis during Q4 2018.”\n",
    "\n",
    "Yelp is a website that offers users an opportunity to leave recommendations for various types of businesses. Yelp also acts a social platform, users are able to have ‘friends’ and rate this ‘friends’ reviews. \n",
    "Each year, Yelp provides a data set that that includes information from local business in 10 metropolitan areas across two countries with the aim of having students research or analyze on this data and share their discoveries.\n",
    "\n",
    "Yelp reviewers leave \"stars\" for the businesses that they are reviewing and each business has an aggragate number of \"stars\" indicating whether they are a good business or an unfavorable business.\n",
    "\n",
    "Utilizing the Yelp dataset, the objective of our project is two fold. First we will look at the the social influence that Yelp Elite users have within the Yelp network by focusing on the links in the bipartite network. Do businesses were Elite users have left recommendations often have more recommendations than other restaurants?  \n",
    "\n",
    "Second, we will build a sentiment analysis of user reviews from the Yelp dataset. We will attempt to find out whether Elite Users tend to leave more positive or negative reviews and based on those reviews, if the business then receives more positive or negative stars. \n",
    "\n",
    "Overall, this analysis will focus on the influence that Elite Yelp users have on business ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages needed to run program\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import numpy as np\n",
    "from collections import Counter \n",
    "\n",
    "# graph viz\n",
    "import plotly\n",
    "import plotly.offline as pyo\n",
    "from plotly.graph_objs import *\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import matplotlib.gridspec as gridspec \n",
    "from wordcloud import *\n",
    "\n",
    "#graph section\n",
    "import networkx as nx\n",
    "\n",
    "#natural language section\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "#machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.svm import LinearSVC\n",
    "import ast\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load each of the datasets from their JSON format to a pandas dataframe. It's important to look at each dataframe and figure out what values are in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>compliment_count</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VaKXUpmWTTWDKbpJ3aQdMw</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-03-27 03:51:24</td>\n",
       "      <td>Great for watching games, ufc, and whatever el...</td>\n",
       "      <td>UPw5DWs_b-e2JRBS-t37Ag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OPiPeoJiv92rENwbq76orA</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-05-25 06:00:56</td>\n",
       "      <td>Happy Hour 2-4 daily with 1/2 price drinks and...</td>\n",
       "      <td>Ocha4kZBHb4JK0lOWvE0sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5KheTjYPu1HcQzQFtm4_vw</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-12-26 01:46:17</td>\n",
       "      <td>Good chips and salsa. Loud at times. Good serv...</td>\n",
       "      <td>jRyO2V1pA4CdVVqCIOPc1Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TkoyGi8J7YFjA6SbaRzrxg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-03-23 21:32:49</td>\n",
       "      <td>The setting and decoration here is amazing. Co...</td>\n",
       "      <td>FuTJWFYm4UKqewaosss1KA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AkL6Ous6A1atZejfZXn1Bg</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-10-06 00:19:27</td>\n",
       "      <td>Molly is definately taking a picture with Sant...</td>\n",
       "      <td>LUlKtaM3nXd-E4N4uOk_fQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  compliment_count                 date  \\\n",
       "0  VaKXUpmWTTWDKbpJ3aQdMw                 0  2014-03-27 03:51:24   \n",
       "1  OPiPeoJiv92rENwbq76orA                 0  2013-05-25 06:00:56   \n",
       "2  5KheTjYPu1HcQzQFtm4_vw                 0  2011-12-26 01:46:17   \n",
       "3  TkoyGi8J7YFjA6SbaRzrxg                 0  2014-03-23 21:32:49   \n",
       "4  AkL6Ous6A1atZejfZXn1Bg                 0  2012-10-06 00:19:27   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0  Great for watching games, ufc, and whatever el...  UPw5DWs_b-e2JRBS-t37Ag  \n",
       "1  Happy Hour 2-4 daily with 1/2 price drinks and...  Ocha4kZBHb4JK0lOWvE0sg  \n",
       "2  Good chips and salsa. Loud at times. Good serv...  jRyO2V1pA4CdVVqCIOPc1Q  \n",
       "3  The setting and decoration here is amazing. Co...  FuTJWFYm4UKqewaosss1KA  \n",
       "4  Molly is definately taking a picture with Sant...  LUlKtaM3nXd-E4N4uOk_fQ  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips = []\n",
    "for line in open('/Users/ntlrsmllghn/Dropbox/Data/Data 620/Final/yelp_dataset/tip.json', 'r'):\n",
    "    tips.append(json.loads(line))\n",
    "\n",
    "tips_df = pd.DataFrame(tips)\n",
    "tips_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1223094, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>attributes</th>\n",
       "      <th>business_id</th>\n",
       "      <th>categories</th>\n",
       "      <th>city</th>\n",
       "      <th>hours</th>\n",
       "      <th>is_open</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>stars</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2818 E Camino Acequia Drive</td>\n",
       "      <td>{'GoodForKids': 'False'}</td>\n",
       "      <td>1SWheh84yJXfytovILXOAQ</td>\n",
       "      <td>Golf, Active Life</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>33.522143</td>\n",
       "      <td>-112.018481</td>\n",
       "      <td>Arizona Biltmore Golf Club</td>\n",
       "      <td>85016</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30 Eglinton Avenue W</td>\n",
       "      <td>{'RestaurantsReservations': 'True', 'GoodForMe...</td>\n",
       "      <td>QXAEGFB4oINsVuTFxEYKFQ</td>\n",
       "      <td>Specialty Food, Restaurants, Dim Sum, Imported...</td>\n",
       "      <td>Mississauga</td>\n",
       "      <td>{'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...</td>\n",
       "      <td>1</td>\n",
       "      <td>43.605499</td>\n",
       "      <td>-79.652289</td>\n",
       "      <td>Emerald Chinese Restaurant</td>\n",
       "      <td>L5R 3E7</td>\n",
       "      <td>128</td>\n",
       "      <td>2.5</td>\n",
       "      <td>ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10110 Johnston Rd, Ste 15</td>\n",
       "      <td>{'GoodForKids': 'True', 'NoiseLevel': 'u'avera...</td>\n",
       "      <td>gnKjwL_1w79qoiV3IC_xQQ</td>\n",
       "      <td>Sushi Bars, Restaurants, Japanese</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>{'Monday': '17:30-21:30', 'Wednesday': '17:30-...</td>\n",
       "      <td>1</td>\n",
       "      <td>35.092564</td>\n",
       "      <td>-80.859132</td>\n",
       "      <td>Musashi Japanese Restaurant</td>\n",
       "      <td>28210</td>\n",
       "      <td>170</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15655 W Roosevelt St, Ste 237</td>\n",
       "      <td>None</td>\n",
       "      <td>xvX2CttrVhyG2z1dFg_0xw</td>\n",
       "      <td>Insurance, Financial Services</td>\n",
       "      <td>Goodyear</td>\n",
       "      <td>{'Monday': '8:0-17:0', 'Tuesday': '8:0-17:0', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>33.455613</td>\n",
       "      <td>-112.395596</td>\n",
       "      <td>Farmers Insurance - Paul Lorenz</td>\n",
       "      <td>85338</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>AZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4209 Stuart Andrew Blvd, Ste F</td>\n",
       "      <td>{'BusinessAcceptsBitcoin': 'False', 'ByAppoint...</td>\n",
       "      <td>HhyxOkGAM07SRYtlQ4wMFQ</td>\n",
       "      <td>Plumbing, Shopping, Local Services, Home Servi...</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>{'Monday': '7:0-23:0', 'Tuesday': '7:0-23:0', ...</td>\n",
       "      <td>1</td>\n",
       "      <td>35.190012</td>\n",
       "      <td>-80.887223</td>\n",
       "      <td>Queen City Plumbing</td>\n",
       "      <td>28217</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          address  \\\n",
       "0     2818 E Camino Acequia Drive   \n",
       "1            30 Eglinton Avenue W   \n",
       "2       10110 Johnston Rd, Ste 15   \n",
       "3   15655 W Roosevelt St, Ste 237   \n",
       "4  4209 Stuart Andrew Blvd, Ste F   \n",
       "\n",
       "                                          attributes             business_id  \\\n",
       "0                           {'GoodForKids': 'False'}  1SWheh84yJXfytovILXOAQ   \n",
       "1  {'RestaurantsReservations': 'True', 'GoodForMe...  QXAEGFB4oINsVuTFxEYKFQ   \n",
       "2  {'GoodForKids': 'True', 'NoiseLevel': 'u'avera...  gnKjwL_1w79qoiV3IC_xQQ   \n",
       "3                                               None  xvX2CttrVhyG2z1dFg_0xw   \n",
       "4  {'BusinessAcceptsBitcoin': 'False', 'ByAppoint...  HhyxOkGAM07SRYtlQ4wMFQ   \n",
       "\n",
       "                                          categories         city  \\\n",
       "0                                  Golf, Active Life      Phoenix   \n",
       "1  Specialty Food, Restaurants, Dim Sum, Imported...  Mississauga   \n",
       "2                  Sushi Bars, Restaurants, Japanese    Charlotte   \n",
       "3                      Insurance, Financial Services     Goodyear   \n",
       "4  Plumbing, Shopping, Local Services, Home Servi...    Charlotte   \n",
       "\n",
       "                                               hours  is_open   latitude  \\\n",
       "0                                               None        0  33.522143   \n",
       "1  {'Monday': '9:0-0:0', 'Tuesday': '9:0-0:0', 'W...        1  43.605499   \n",
       "2  {'Monday': '17:30-21:30', 'Wednesday': '17:30-...        1  35.092564   \n",
       "3  {'Monday': '8:0-17:0', 'Tuesday': '8:0-17:0', ...        1  33.455613   \n",
       "4  {'Monday': '7:0-23:0', 'Tuesday': '7:0-23:0', ...        1  35.190012   \n",
       "\n",
       "    longitude                             name postal_code  review_count  \\\n",
       "0 -112.018481       Arizona Biltmore Golf Club       85016             5   \n",
       "1  -79.652289       Emerald Chinese Restaurant     L5R 3E7           128   \n",
       "2  -80.859132      Musashi Japanese Restaurant       28210           170   \n",
       "3 -112.395596  Farmers Insurance - Paul Lorenz       85338             3   \n",
       "4  -80.887223              Queen City Plumbing       28217             4   \n",
       "\n",
       "   stars state  \n",
       "0    3.0    AZ  \n",
       "1    2.5    ON  \n",
       "2    4.0    NC  \n",
       "3    5.0    AZ  \n",
       "4    4.0    NC  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business = []\n",
    "for line in open('/Users/ntlrsmllghn/Dropbox/Data/Data 620/Final/yelp_dataset/business.json', 'r'):\n",
    "    business.append(json.loads(line))\n",
    "business_df = pd.DataFrame(business)\n",
    "business_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192609, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = []\n",
    "for line in open('/Users/ntlrsmllghn/Dropbox/Data/Data 620/Final/yelp_dataset/review.json', 'r'):\n",
    "    review.append(json.loads(line))\n",
    "review_df = pd.DataFrame(review)\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = []\n",
    "for line in open('/Users/ntlrsmllghn/Dropbox/Data/Data 620/Final/yelp_dataset/user.json', 'r'):\n",
    "    user.append(json.loads(line))\n",
    "user_df = pd.DataFrame(user)\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see the review for each business, we must combine the business dataframe and review dataframe. The common column that they both share is the `business_id` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_review_df = pd.merge(business_df, review_df, how='inner', on='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business_review_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rows for this dataframe is equal to the review column lenght. Our merge was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "business_review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_review_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since some of the columns shared the same names, we had to rename columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_review_df.rename(columns={'name_x': 'business_name', 'stars_x':'average_stars','stars_y': 'reviewer_star'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_review_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find out more about the elite user rewiews, we have to merge a third dataframe to the business and review dataframes. The common column that these dataframe share is `user_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.merge(business_review_df, user_df, how='inner', on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of columns within this dataframe that are not useful for our analysis, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = master_df.drop(columns=['compliment_cool', 'compliment_funny', 'compliment_hot', 'compliment_list', 'compliment_more',\n",
    "                        'compliment_note', 'compliment_photos', 'compliment_cute', 'compliment_plain', 'compliment_profile',\n",
    "                       'compliment_writer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, there are columns with the same name, so we will rename these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.rename(columns={'review_count_x': 'business_review_cnt', 'average_stars_x':'business_avg_stars', 'cool_x':'cool_business_review',\n",
    "                          'funny_x': 'funny_business_review', 'useful_x':'useful_business_review', 'name_y':'user_name', \n",
    "                          'average_stars_y': 'user_avg_stars', 'cool_y':'cool_user_reviews', 'funny_y':'funny_user_reviews',\n",
    "                          'review_count_y':'user_review_cnt', 'useful_y':'useful_user_reviews'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking the shape of the new master dataframe, we find that this dataframe is the same lenght as the `business_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded all the data, let's do an exploratory analysis of our data.\n",
    "\n",
    "First, we should find out what type of businesses are reviewed by Yelp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = set()\n",
    "for business in business_df['categories'][business_df['categories'].notnull()].str.split(','):\n",
    "    business = [x.strip(' ') for x in business]\n",
    "    category_list = set().union(business, category_list)\n",
    "category_list = list(category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_count = []\n",
    "for cat in category_list:\n",
    "    category_count.append([cat,business_df['categories'].str.contains(cat).sum()])\n",
    "\n",
    "names = ['category_name','category_count']\n",
    "category_df = pd.DataFrame(data=category_count, columns=names)\n",
    "category_df.sort_values(\"category_count\", inplace=True, ascending=False)\n",
    "category_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(8, 8))\n",
    "labels=category_df['category_name'][category_df['category_count']>10000]\n",
    "category_df['category_count'][category_df['category_count']>10000].plot.bar( align='center', alpha=0.5)\n",
    "y_pos = np.arange(len(labels))\n",
    "#plt.yticks(y_pos, labels)\n",
    "plt.xticks(y_pos, labels)\n",
    "plt.xlabel('Business Categories')\n",
    "plt.ylabel('Categories Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restaurants, Food, and Shopping are the top three categories reviewed on Yelp. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which cities are featured in this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=business_df['city'].value_counts()\n",
    "x=x.sort_values(ascending=False)\n",
    "x=x.iloc[0:20]\n",
    "plt.figure(figsize=(16,4))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Which city has the most reviews?\")\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=45)\n",
    "plt.ylabel('# businesses', fontsize=12)\n",
    "plt.xlabel('City', fontsize=12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cities = business_df.city.value_counts()\n",
    "top_cities.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this dataset, Las Vegas, Toronto, and Phoenix have the most reviews. \n",
    "\n",
    "It would be interesting to see what the average number of stars these cities have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_business_reviews = business_df[['city', 'review_count', 'stars']].groupby(['city']).\\\n",
    "agg({'review_count': 'sum', 'stars': 'mean'}).sort_values(by='review_count', ascending=False)\n",
    "city_business_reviews.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which business have the most reviews? Let's take a look at the top 25 most reviewed businesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df[['name', 'review_count', 'city', 'stars']].sort_values(ascending=False, by=\"review_count\")[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these businesses are located in Las Vegas. Likely, this is because there are more reviews in the dataset for businesses in Las Vegas.\n",
    "\n",
    "Let's see how stars are distributed within this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=business_df['stars'].value_counts()\n",
    "x=x.sort_index()\n",
    "#plot\n",
    "plt.figure(figsize=(8,4))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Star Rating Distribution\")\n",
    "plt.ylabel('# of businesses', fontsize=12)\n",
    "plt.xlabel('Star Ratings ', fontsize=12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.5 to 5 stars make up most of the reviews. Let's see if that means that there will be more positive sentiment than a negative one.\n",
    "\n",
    "Is there any correlation between the number of stars and the length of the review? Or how useful/cool/or funny the reviews are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['text length'] = review_df['text'].apply(len)\n",
    "g = sns.FacetGrid(data=review_df, col='stars')\n",
    "g.map(plt.hist, 'text length', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = review_df.groupby('stars').mean()\n",
    "stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the matrix, looks like the 1-star and 2-star ratings have much longer text - maybe text length won’t be such a useful feature to consider after all.\n",
    "\n",
    "Looking at the matrix, funny is strongly correlated with useful, and useful seems strongly correlated with text length. We can also see a negative correlation between cool and the other three features. Maybe funny reviews are longer than useful and cool reviews.\n",
    "\n",
    "Which businesses have the top rated reviews in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['name'] = review_df['business_id'].map(business_df.set_index('business_id')['name'])\n",
    "top_rated = review_df.name.value_counts().index[:20].tolist()\n",
    "top_rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review_top = review_df.loc[review_df['name'].isin(top_rated)]\n",
    "df_review_top.groupby(df_review_top.name)['stars'].mean().sort_values(ascending=True).plot(kind='barh',figsize=(12, 10))\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Top rated businesses on Yelp',fontsize=12)\n",
    "plt.ylabel('Business names', fontsize=12)\n",
    "plt.xlabel('Ratings', fontsize=12) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a sense of what the business, reviews, and distribution of stars look like, we want to start looking at the social network of Yelp Reviews and Businesses.\n",
    "\n",
    "\n",
    "<b>Social Network Analysis of Yelp users</b>\n",
    "\n",
    "We used the user dataframe, more specifically user_id and friends to define the nodes and the edges of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert string to timestampe\n",
    "user_df['joined']= pd.to_datetime(user_df['yelping_since'])\n",
    "#group by year and count occurrences\n",
    "yearGrouping =user_df.groupby(user_df['joined'].map(lambda x : x.year))['yelping_since'].count()\n",
    "user_df['number of Friends'] = pd.to_numeric(user_df['friends'], errors='coerce').fillna(0)\n",
    "user_df['number of Friends']=user_df['number of Friends'].astype(np.int64)\n",
    "user_df['elite']=pd.to_numeric(user_df['elite'],errors='coerce').fillna(0)\n",
    "user_df['elite']=user_df['elite'].astype(np.int64)                                                                   \n",
    "#user_df['target'] = user_df['elite']!='[]'\n",
    "print(user_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(yearGrouping)\n",
    "plt.figure()\n",
    "plt.scatter(user_df['average_stars'],user_df['review_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Looking at the entire user data</b>\n",
    "\n",
    "1. From the above figure we can see that users joining inclnation has started from around 2006 and peaked somewhere between 2014 and 2016 and it's currently declining\n",
    "2. The second scatter plot depicts the review count and the number of stars. We can see that somewhere between 3.5 and 4 stars people leave more reviews.\n",
    "\n",
    "\n",
    "<b>Creating a subset of users</b>\n",
    "\n",
    "Since graphing the entire data would require a lot of memory and it would look super messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset users who have atleast 1 friend\n",
    "subset_users=user_df[user_df['friends']!='None']\n",
    "#user has given atleast 10 reviews\n",
    "subset_users=subset_users[subset_users['review_count']>=10]\n",
    "#subset_users=subset_users.sort_values('review_count',ascending=False)\n",
    "\n",
    "subset_users['list_friends']=subset_users[\"friends\"].apply(lambda x: str(x).split(','))\n",
    "\n",
    "subset_users=subset_users[['user_id','list_friends']]\n",
    "#stopping at 6k due to space constraints\n",
    "subset_users=subset_users.iloc[0:6000]\n",
    "res = subset_users.set_index(['user_id'])['list_friends'].apply(pd.Series).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_data=res.reset_index()\n",
    "#checking the dataframe\n",
    "network_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the column name to suit nx import\n",
    "network_data.columns=['source','level_1','target']\n",
    "\n",
    "# Considering each (user_id,friend) pair as an edge of a graph, constructing the graph\n",
    "graph=nx.from_pandas_edgelist(network_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(graph))\n",
    "#check density\n",
    "print(\"The density of the graph is \",nx.density(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Network Density</b>\n",
    "\n",
    "\"Network density is a measure of the proportion of possible ties which are actualized among the members of a network. Dense social networks, especially coupled with strong boundaries segregating the group from others, can enforce communal norms so that social pressures for conformity can inhibit creativity, which necessarily contains an element of deviance. Small dense networks may develop ‘groupthink’ where conformity of ideas is highly valued and normatively enforced. This inhibits creativity within the group.\"--Katherine Giuffre, Cultural Productons in Networks\n",
    "\n",
    "\n",
    "We can see that the density of the graph is not very high but again we are not portraying the entire network due to memory contraints. \"From an academic perspective graph density would be defined as the ratio of the number of edges and the number of possible edges.\" For the purposes of our project it would be intereting to see who are the most influetial Yelp users and draw a graph that depicts all the connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets take a single town's population and make a graph out of those users\n",
    "# since we dont have people and location together\n",
    "# Mapping businesses of a location to reviews and then to users and then finding their friends network\n",
    "\n",
    "subset=business_df[business_df.city=='Cleveland']\n",
    "subset=pd.merge(subset,review_df,how='inner',on='business_id')\n",
    "subset_users=subset.user_id.unique()\n",
    "\n",
    "subset_users=pd.DataFrame(subset_users,columns=['user_id'])\n",
    "subset_users=pd.merge(subset_users,user_df,how='inner',on='user_id')\n",
    "\n",
    "# create friend list\n",
    "subset_users['list_friends']=subset_users[\"friends\"].apply(lambda x: str(x).split(','))\n",
    "subset_users['count_friends']=subset_users[\"list_friends\"].apply(lambda x: len(x))\n",
    "\n",
    "#check\n",
    "subset_users.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset subset_users reflects the data from only Cleveland and we will attempt to graph that subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_users_list=subset_users[['user_id','list_friends']]\n",
    "network_data = subset_users_list.set_index(['user_id'])['list_friends'].apply(pd.Series).stack()\n",
    "network_data=network_data.reset_index()\n",
    "#changing the column name to suit nx import\n",
    "network_data.columns=['source','level_1','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering each (user_id,friend) pair as an edge of a graph, constructing the graph\n",
    "graph=nx.from_pandas_edgelist(network_data)\n",
    "print(nx.info(graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The top/ most influential Yelp users</b>\n",
    "\n",
    "We wanted to look into the most influential people and used the heapq library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use degree-centrality to find out influencers in the selected region\n",
    "import heapq  # for getting top n number of things from list,dict\n",
    "x=nx.degree_centrality(graph)\n",
    "#Creating a subset again as we cant handle 70k nodes, unfortunately.\n",
    "\n",
    "#Using heapq to find the 200 most connected nodes (ie) people with the most connections\n",
    "elite=heapq.nlargest(200, x, key=x.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "heapq.nlargest--Return a list with the n largest elements from the dataset defined by iterable. key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_sub_graph=graph.subgraph(elite)\n",
    "\n",
    "# Check for isolates ( nodes with no edges (ie) users without friends in the sub-graph)\n",
    "# graph=graph.remove_nodes_from(nx.isolates(graph))\n",
    "list_of_nodes_to_be_removed=[x for x in nx.isolates(elite_sub_graph)]\n",
    "\n",
    "# remove the selected isolates from the main graph\n",
    "graph.remove_nodes_from(list_of_nodes_to_be_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(elite_sub_graph))\n",
    "#check density\n",
    "print(\"The density of the graph is \",nx.density(elite_sub_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the layout\n",
    "pos = nx.spring_layout(elite_sub_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.title(\"Cleveland's elite graph\")\n",
    "nx.draw(elite_sub_graph, pos=pos, node_size=0.05, width=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the graph that there are 2 users that are the \"central\" nodes in this network. Kamada Kawai algorythm-- \"The system tries to find a balance between the “springs”vertices whose edges have small desired distance tend to move in groups away from the more dissimilar vertices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#creating a larger Kamada Kawai layout \n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title(\"Plot of Cleveland's community : Kamada Kawai layout\")\n",
    "pos2=nx.kamada_kawai_layout(elite_sub_graph)\n",
    "nx.draw(elite_sub_graph, pos=pos, node_size=0.7, width=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting communities within the Cleveland's subgraph elite users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_generator = community.girvan_newman(elite_sub_graph)\n",
    "top_level_communities = next(communities_generator)\n",
    "next_level_communities = next(communities_generator)\n",
    "print(len(top_level_communities))\n",
    "#sorted(map(sorted, top_level_communities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three Communities have been identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(top_level_communities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.axes=False\n",
    "plt.title(\"Cleveland's graph of the 200 most influential users\" , fontsize=20)\n",
    "nx.draw_networkx(elite_sub_graph, pos = pos2,cmap = plt.get_cmap(\"jet\"), node_size = 0.9, with_labels = False,scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if the graph is connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_connected(elite_sub_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the function betweenness_centrality() to compute the centrality of each node. This function returns a list of tuples, one for each node, and each tuple contains the label of the node and the centrality value. We can use this information in order to trim the original network and keep only the most important nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=elite_sub_graph\n",
    "def most_important(G):\n",
    " \"\"\" returns a copy of G with\n",
    "     the most important nodes\n",
    "     according to the pagerank \"\"\" \n",
    " ranking = nx.betweenness_centrality(G).items()\n",
    " print(ranking)\n",
    " r = [x[1] for x in ranking]\n",
    " m = sum(r)/len(r) # mean centrality\n",
    " t = m*3 # threshold, we keep only the nodes with 3 times the mean\n",
    " Gt = G.copy()\n",
    " for k, v in ranking:\n",
    "  if v < t:\n",
    "   Gt.remove_node(k)\n",
    " return Gt\n",
    "\n",
    "Gt = most_important(G) # trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use the original network and the trimmed one to visualize the network as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import show\n",
    "# create the layout\n",
    "pos = nx.spring_layout(G)\n",
    "# draw the nodes and the edges (all)\n",
    "nx.draw_networkx_nodes(G,pos,node_color='b',alpha=0.9,node_size=8)\n",
    "nx.draw_networkx_edges(G,pos,alpha=0.1)\n",
    "\n",
    "# draw the most important nodes with a different style\n",
    "nx.draw_networkx_nodes(Gt,pos,node_color='r',alpha=0.4,node_size=254)\n",
    "# also the labels this time\n",
    "nx.draw_networkx_labels(Gt,pos,font_size=12,font_color='b')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>\n",
    "\n",
    "We can clearly see the 2 most influential users within this network. One of the main qualities of an elite user on Yelp is connectivity and we were interested in looking to see to what extend this quality is really preveant in the network. Our results show that claim to be true-- the most inflential users are highly connected to the rest of the users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sentiment Analysis</b>\n",
    "\n",
    "Now ...\n",
    "\n",
    "Let's see if elite users are more likely to leave positive or negative reviews on businesses and if this seems to impact the number of stars that a business has.\n",
    "\n",
    "First, we subset elite users from the master dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_user_df = master_df[master_df['elite'].str.contains(\"201\", na = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_user_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_user_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_user_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will subset the `elite_users_df` by average business stars. If a business has less than 3 stars, we will assume that the business will have negative reviews and if the business has 3 stars or above, we will classify the business as having positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=elite_user_df['business_avg_stars'].value_counts()\n",
    "x=x.sort_index()\n",
    "#plot\n",
    "plt.figure(figsize=(8,4))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Star Rating Distribution for Businesses Reviewed by Elite Users\")\n",
    "plt.ylabel('# of reviews', fontsize=12)\n",
    "plt.xlabel('Star Ratings ', fontsize=12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviws = elite_user_df[elite_user_df[\"business_avg_stars\"]<=2]\n",
    "positive_reviews = elite_user_df[elite_user_df[\"business_avg_stars\"]>=3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset has 1559043 reviews, I'm going to reduce the dataset to 50000. This is for the sake of my computer's processing abilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_user_df = elite_user_df[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An initial step in text and sentiment classification is pre-processing. A significant amount of techniques is applied to data in order to reduce the noise of text, reduce dimensionality, and assist in the improvement of classification effectiveness. The most popular techniques include:\n",
    "\n",
    "Remove numbers,\n",
    "Stemming,\n",
    "Part of speech tagging,\n",
    "Remove punctuation,\n",
    "Lowercase,\n",
    "Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    x = re.sub('[^a-z\\s]', '', x.lower())                  \n",
    "    x = [w for w in x.split() if w not in set(stopwords)]  \n",
    "    return ' '.join(x)\n",
    "\n",
    "elite_user_df['processed_text'] = elite_user_df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find out the sentiment of most user reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(x):\n",
    "    sentiment = TextBlob(x)\n",
    "    return sentiment.sentiment.polarity\n",
    "\n",
    "elite_user_df['text_sentiment'] = elite_user_df['processed_text'].apply(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, Let's plot the user sentiment polarity to understand what type of reviews we're going to be training and see if these positive or negative reviews have an impact on predicting number of stars a business has. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elite_user_df['sentiment'] = ''\n",
    "elite_user_df['sentiment'][elite_user_df['text_sentiment'] > 0] = 'positive'\n",
    "elite_user_df['sentiment'][elite_user_df['text_sentiment'] < 0] = 'negative'\n",
    "elite_user_df['sentiment'][elite_user_df['text_sentiment'] == 0] = 'neutral'\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.countplot(elite_user_df['sentiment'])\n",
    "plt.title('Review Sentiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a classification algorithm will need some sort of feature vector in order to perform the classification task. The simplest way to convert a corpus to a vector format is the bag-of-words approach, where each unique word in a text will be represented by one number.\n",
    "\n",
    "Let’s create a function that will split a review into individual words, return a list, and remove stop words (such as “the”, “a”, “an”, etc.). To do this, we can take advantage of the NLTK library. The function below removes punctuation, stopwords, and returns a list of the remaining words, or tokens.\n",
    "\n",
    "First, let's get the average business stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_class = elite_user_df[(elite_user_df['business_avg_stars'] == 1) | (elite_user_df['business_avg_stars'] == 2)| (elite_user_df['business_avg_stars'] == 3)|(elite_user_df['business_avg_stars'] == 4)| (elite_user_df['business_avg_stars'] == 5)]\n",
    "review_class.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will add assign `X` and `Y` to the two variables we want to use in our sentiment analysis. The text of a review by elite user and the average business stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = review_class['text']\n",
    "Y = review_class['business_avg_stars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function for cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(review):\n",
    "    clean_data = [char for char in review if char not in string.punctuation]\n",
    "    clean_data = ''.join(clean_data)\n",
    "    \n",
    "    return [word for word in clean_data.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words to convert the text documents into a matrix of token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_word_transformer = CountVectorizer(analyzer=clean_review).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find out the shape of the sparse matrix, the amount of non-zero occurances, and the denistory of the non-zero values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_word_transformer.transform(X)\n",
    "print('Shape of Sparse Matrix: ', X.shape)\n",
    "print('Amount of Non-Zero occurrences: ', X.nnz)\n",
    "# Percentage of non-zero values\n",
    "density = (100.0 * X.nnz / (X.shape[0] * X.shape[1]))\n",
    "print(\"Density: {}\".format((density)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function to classify if a review is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(sent):\n",
    "    if sent <= 2:\n",
    "        print(\"negative\")\n",
    "    else:\n",
    "        print(\"positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's transform a count matrix to a normalized tf or tf-idf representation\n",
    "\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()  \n",
    "X = tfidfconverter.fit_transform(X).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to split our dataset into train and test samples, 70% of the dataset 30% of the dataset is the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try a few different approaches to predicting if an elite user leaves a positive or negative review, what the average stars of the restaurant will be. \n",
    "\n",
    "The first approach will be to use the naive bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naiveBayes = MultinomialNB()\n",
    "naiveBayes.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = naiveBayes.predict(X_test)\n",
    "print (\"Prediction using Naive Bayes Model\")\n",
    "print(prediction)\n",
    "ax = sns.countplot(prediction)\n",
    "plt.title('Naive Bayes Model Review Sentiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a list that keeps track of our f1 scores for each of the models that we run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_scr  = []\n",
    "f1_scr.append(f1_score(Y_test,prediction,average='micro')*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next model will be a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC()\n",
    "svc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = svc.predict(X_test)\n",
    "print (\"Prediction using Linear SVC model\\n\")\n",
    "print(prediction)\n",
    "ax = sns.countplot(prediction)\n",
    "plt.title('Linear SVC model Review Sentiments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append our f1 score list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scr.append(f1_score(Y_test,prediction,average='micro')*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our third model will be a Logistical Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LogReg = LogisticRegression()\n",
    "LogReg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = LogReg.predict(X_test)\n",
    "\n",
    "print (\"Prediction using Logistic Regression model\\n\")\n",
    "print(prediction)\n",
    "ax = sns.countplot(prediction)\n",
    "plt.title('Logistic Regression Prediction of Review Sentiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scr.append(f1_score(Y_test,prediction,average='micro')*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's try a Random Forest Classifier model to predict if an elite user leaves a review, will it have an impact of the overall stars a business has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=18)\n",
    "rf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = rf.predict(X_test)\n",
    "print (\"Prediction using Random Forest Classifier model\\n\")\n",
    "print(prediction)\n",
    "ax = sns.countplot(prediction)\n",
    "plt.title('Random Forest Classifier model Review Sentiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scr.append(f1_score(Y_test,prediction,average='weighted')*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line1 = plt.plot (\n",
    "          ['Naive Bayes','Linear SVC','LogisticRegression','RandomForest Classifier'],f1_scr ,'--o',alpha=0.7)\n",
    "plt.title(\"Model Evaluation\")\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('Models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_scr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the F1 score, we see that elite users sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
